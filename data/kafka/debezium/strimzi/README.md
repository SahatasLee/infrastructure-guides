# Debezium on Kubernetes (Strimzi)

> **Description:** Guide to deploying Debezium as a Kafka Connect cluster using Strimzi Operator.
> **Last Updated:** 2025-12-17

## üìã Overview

‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Debezium ‡∏ö‡∏ô Kubernetes ‡∏ú‡πà‡∏≤‡∏ô Strimzi ‡∏°‡∏µ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÜ ‡∏Ñ‡∏∑‡∏≠:
1. **Build Kafka Connect Image**: ‡∏™‡∏£‡πâ‡∏≤‡∏á Docker Image ‡∏ó‡∏µ‡πà‡∏°‡∏µ Kafka Connect ‡πÅ‡∏•‡∏∞ Debezium Plugins
2. **Deploy Kafka Connect**: ‡∏™‡∏£‡πâ‡∏≤‡∏á `KafkaConnect` CR ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Image ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á
3. **Create Connector**: ‡∏™‡∏£‡πâ‡∏≤‡∏á `KafkaConnector` CR ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

---

## üõ†Ô∏è Step 1: Build Custom Image

Debezium ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á Plugin (JAR files) ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏™‡πà‡πÉ‡∏ô Kafka Connect ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Docker Image ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà

**Dockerfile:**
```dockerfile
FROM quay.io/strimzi/kafka:0.38.0-kafka-3.6.0
USER root:root
RUN mkdir -p /opt/kafka/plugins/debezium
COPY ./plugins/ /opt/kafka/plugins/debezium/
USER 1001
```

*‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏õ download Debezium connector plugin (zip) ‡∏à‡∏≤‡∏Å [Debezium Website](https://debezium.io/releases/) ‡∏°‡∏≤‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏™‡πà folder `plugins` ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ Download ‡πÉ‡∏ô Dockerfile ‡πÄ‡∏•‡∏¢‡∏Å‡πá‡πÑ‡∏î‡πâ*

**Alternative Dockerfile (Download Direct):**
```dockerfile
FROM quay.io/strimzi/kafka:0.38.0-kafka-3.6.0
USER root:root
RUN microdnf install -y curl tar gzip && \
    mkdir -p /opt/kafka/plugins/debezium && \
    curl -L https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/2.5.0.Final/debezium-connector-postgres-2.5.0.Final-plugin.tar.gz | tar -xz -C /opt/kafka/plugins/debezium
USER 1001
```

build ‡πÅ‡∏•‡∏∞ push ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Registry ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:
```bash
docker build -t my-registry/kafka-connect-debezium:latest .
docker push my-registry/kafka-connect-debezium:latest
```

---

## üöÄ Step 2: Deploy Kafka Connect

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `kafka-connect.yaml`:

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  name: my-connect-cluster
  namespace: kafka
  annotations:
    strimzi.io/use-connector-resources: "true" # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á Connector ‡∏ú‡πà‡∏≤‡∏ô CR ‡πÑ‡∏î‡πâ
spec:
  version: 3.6.0
  replicas: 1
  bootstrapServers: my-cluster-kafka-bootstrap:9092
  image: my-registry/kafka-connect-debezium:latest # Image ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤ Build
  config:
    group.id: connect-cluster
    offset.storage.topic: connect-cluster-offsets
    config.storage.topic: connect-cluster-configs
    status.storage.topic: connect-cluster-status
    # ... converters configs
    key.converter: org.apache.kafka.connect.json.JsonConverter
    value.converter: org.apache.kafka.connect.json.JsonConverter
    key.converter.schemas.enable: false
    value.converter.schemas.enable: false
```

---

## üîå Step 3: Create Connector

‡πÉ‡∏ä‡πâ Custom Resource `KafkaConnector` ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏¢‡∏¥‡∏á API ‡πÄ‡∏≠‡∏á:

### PostgreSQL
‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÉ‡∏ô `connector.yaml`

### MySQL
‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô `connector-mysql.yaml`

### SQL Server
‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô `connector-sqlserver.yaml`

**‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:** ‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤ connection config (hostname, user, password) ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Database ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì

---

## ‚úÖ Verification

```bash
# Check Kafka Connect Status
kubectl get kafkaconnect -n kafka


---

## üß† Deep Dive: How it works

‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Connector ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Config ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

### 1. The Configuration Explained

‡∏°‡∏≤‡∏î‡∏π‡πÑ‡∏™‡πâ‡πÉ‡∏ô‡∏Ç‡∏≠‡∏á `connector-psql.yaml` (‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏∑‡πà‡∏ô‡πÜ) ‡∏Å‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö:

```yaml
spec:
  class: io.debezium.connector.postgresql.PostgresConnector 
  tasksMax: 1
  config:
    database.hostname: postgres-service  # IP ‡∏´‡∏£‡∏∑‡∏≠ Service Name ‡∏Ç‡∏≠‡∏á Database
    database.port: 5432
    database.user: postgres
    database.password: postgres
    database.dbname: postgres
    
    # üéØ Topic Prefix: ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å! ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô prefix ‡∏Ç‡∏≠‡∏á Topic ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á
    topic.prefix: dbserver1
    
    # üìã Table Whitelist: ‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Table ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (Format: schema.table)
    table.include.list: public.customers
    
    # üß© Plugin Name: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Postgres ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ pgoutput (Built-in decoding logic)
    plugin.name: pgoutput
```

### 2. How Topics are Created (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!)

Debezium ‡∏à‡∏∞ **‡∏™‡∏£‡πâ‡∏≤‡∏á Kafka Topic ‡πÉ‡∏´‡πâ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÜ Table ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏õ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Naming Convention ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

> **Format:** `<topic.prefix>.<schema>.<table_name>`

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**
- `topic.prefix` = **dbserver1**
- schema = **public**
- table = **customers**

üëâ Kafka Topic ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏Ñ‡∏∑‡∏≠: `dbserver1.public.customers`

### 3. Message Structure (Value)

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Database ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á (Insert/Update/Delete) Debezium ‡∏à‡∏∞ Produce message ‡πÑ‡∏õ‡∏•‡∏á Topic
‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Value) ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

```json
{
  "schema": { ... }, 
  "payload": {
    "before": {           // ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• "‡∏Å‡πà‡∏≠‡∏ô" ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á (‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô null ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô INSERT)
      "id": 101,
      "email": "old@example.com"
    },
    "after": {            // ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• "‡∏´‡∏•‡∏±‡∏á" ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á (‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô null ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô DELETE)
      "id": 101,
        "email": "new@example.com"
    },
    "source": { ... },    // 
    "op": "u",            // Operation type: c=create, u=update, d=delete, r=read (snapshot)
    "ts_ms": 1638345678   // timestamp
  }
}
```

### 4. ‡∏™‡∏£‡∏∏‡∏õ Flow ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô

1. **Connector Start:** Debezium ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏¢‡∏±‡∏á Database
2. **Snapshot (Optional):** ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å ‡∏°‡∏±‡∏ô‡∏à‡∏∞ Select ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Event (op=`r`)
3. **Stream:** ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∞ Monitor Transaction Log (WAL)
4. **Produce:** ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠ change -> ‡∏™‡∏£‡πâ‡∏≤‡∏á JSON -> ‡∏¢‡∏¥‡∏á‡∏•‡∏á Topic `prefix.schema.table`
5. **Consume:** ‡πÉ‡∏ä‡πâ Kafka Consumer ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÉ‡∏ä‡πâ

### 5. FAQ: ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Topic ‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°? (Custom Topics)

**‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö!** ‡πÅ‡∏•‡∏∞ **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏≥** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Production ‡∏î‡πâ‡∏ß‡∏¢

#### Scenario A: ‡∏≠‡∏¢‡∏≤‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î Config ‡∏Ç‡∏≠‡∏á Topic (Partitions, Retention)
Debezium ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Topic ‡πÉ‡∏´‡πâ Auto ‡∏Å‡πá‡∏à‡∏£‡∏¥‡∏á ‡πÅ‡∏ï‡πà‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô Default Config (1 partition) ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏û‡∏≠
**‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≥:** ‡∏™‡∏£‡πâ‡∏≤‡∏á `KafkaTopic` CR ‡∏£‡∏≠‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏•‡∏¢ **‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà Debezium ‡∏à‡∏∞‡πÉ‡∏ä‡πâ** (`prefix.schema.table`)

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: dbserver1.public.customers  # <-- ‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πä‡∏∞!
  labels:
    strimzi.io/cluster: my-cluster
spec:
  partitions: 3          # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Partition ‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ
  replicas: 3
  config:
    retention.ms: 604800000  # 7 Days
    cleanup.policy: compact  # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CDC!
```

#### Scenario B: ‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ Topic ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ (‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤ prefix.schema.table)
‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ Topic ‡∏ä‡∏∑‡πà‡∏≠ `crm.users` ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô `dbserver1.public.customers`
**‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≥:** ‡πÉ‡∏ä‡πâ **SMT (Single Message Transform)** ‡∏ä‡∏∑‡πà‡∏≠ `ByLogicalTableRouter`

‡πÄ‡∏û‡∏¥‡πà‡∏° Config ‡∏ô‡∏µ‡πâ‡∏•‡∏á‡πÉ‡∏ô `KafkaConnector`:

```yaml
config:
  # ... config ‡πÄ‡∏î‡∏¥‡∏° ...
  
  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ Topic
  transforms: RerouteTopic
  transforms.RerouteTopic.type: io.debezium.transforms.ByLogicalTableRouter
  transforms.RerouteTopic.topic.regex: dbserver1.public.(.*)
  transforms.RerouteTopic.topic.replacement: crm.$1
```

‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ Topic ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô `crm.customers` ‡πÅ‡∏ó‡∏ô
  
---

### 6. FAQ: ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö CloudNativePG (CNPG) ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°?

**‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö!** ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°‡∏°‡∏≤‡∏Å‡πÉ‡∏ô Kubernetes

‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô‡∏ù‡∏±‡πà‡∏á **CNPG Cluster YAML**:

1. **‡πÄ‡∏õ‡∏¥‡∏î WAL Level**: ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ `postgresql.conf` ‡∏ú‡πà‡∏≤‡∏ô `spec.postgresql`
2. **Connection**: ‡πÉ‡∏´‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Service RW (`-rw`)

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á CNPG Cluster:**
```yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: my-pg-cluster
spec:
  # ... options ...
  postgresql:
    parameters:
      wal_level: logical  # üëà ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ!
      max_replication_slots: "10" 
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Debezium Config:**
```yaml
config:
  database.hostname: my-pg-cluster-rw # üëà ‡πÉ‡∏ä‡πâ Service RW
  database.port: 5432
  database.user: streaming_replica    # ‡∏´‡∏£‡∏∑‡∏≠ user ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå replication
  # ...
